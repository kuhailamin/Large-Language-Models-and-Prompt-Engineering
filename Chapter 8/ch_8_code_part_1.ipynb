{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDNKwl_kv8Z0",
        "outputId": "69de21d1-a95c-4888-ce47-eabeee8d75fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.579\n"
          ]
        }
      ],
      "source": [
        "# Install and import NLTK's BLEU scorer\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference and candidate sentences\n",
        "reference = \"the cat is on the mat\"\n",
        "candidate = \"the cat is on mat\"\n",
        "\n",
        "# Tokenize the sentences by words\n",
        "ref_tokens = reference.split()            # e.g., ['the','cat','is','on','the','mat']\n",
        "cand_tokens = candidate.split()          # e.g., ['the','cat','is','on','mat']\n",
        "\n",
        "# Calculate BLEU score (up to 4-gram by default)\n",
        "bleu_score = sentence_bleu([ref_tokens], cand_tokens)\n",
        "print(f\"BLEU score: {bleu_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Reference and candidate (e.g., summary sentences)\n",
        "reference = \"the cat is on the mat\"\n",
        "candidate = \"the cat is mat\"\n",
        "\n",
        "# Tokenize into words\n",
        "ref_tokens = reference.split()\n",
        "cand_tokens = candidate.split()\n",
        "\n",
        "# Count unigrams in each\n",
        "ref_counts = Counter(ref_tokens)\n",
        "cand_counts = Counter(cand_tokens)\n",
        "\n",
        "# Count overlap â€“ for each unique word, how many times it appears in both\n",
        "overlap = 0\n",
        "for word, cnt in cand_counts.items():\n",
        "    overlap += min(cnt, ref_counts.get(word, 0))\n",
        "\n",
        "# Compute precision, recall, and F1 for unigrams\n",
        "precision = overlap / len(cand_tokens) if cand_tokens else 0.0\n",
        "recall    = overlap / len(ref_tokens) if ref_tokens else 0.0\n",
        "if precision + recall > 0:\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "else:\n",
        "    f1 = 0.0\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROUGE-1 F1: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaQvTYgZ5zHy",
        "outputId": "e0e2b9e4-a816-485a-a948-aa9427c8e4c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall: 0.67\n",
            "ROUGE-1 F1: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')  # Ensure WordNet is available for synonym matching\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n01bgIIS_6Bd",
        "outputId": "f483f6ae-11a3-4519-b7c2-576cb4a6fad8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Reference (human-written sentence)\n",
        "reference = \"The boy quickly ran to school to avoid being late\"\n",
        "\n",
        "# Hypothesis (model-generated sentence)\n",
        "hypothesis = \"The kid hurried to the school so he wouldn't be late\"\n",
        "\n",
        "# Compute METEOR score (range 0 to 1)\n",
        "score = meteor_score([reference.split()], hypothesis.split())\n",
        "print(f\"METEOR score: {score:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrii7QfvANlE",
        "outputId": "ba9f8a6e-8cb7-47c1-fcc7-021886fa515e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Suppose our model predicts the following probabilities for each token in a sequence:\n",
        "probs = [0.1, 0.5, 0.2, 0.8]  # Example probabilities P_hat(y_j) for j=1..4\n",
        "\n",
        "# Ensure none of the probabilities are zero (perplexity is infinite if so)\n",
        "assert all(p > 0 for p in probs), \"Probabilities must be > 0\"\n",
        "\n",
        "# Calculate average negative log2-probability\n",
        "avg_neg_log2 = -sum(math.log2(p) for p in probs) / len(probs)\n",
        "\n",
        "# Calculate perplexity as 2^(average_negative_log2_prob)\n",
        "perplexity = 2 ** avg_neg_log2\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm08NCarFehz",
        "outputId": "5373c0df-937b-4505-825d-6a8bcda49af6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 3.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# True labels and model predictions for 5 examples (1 = positive class, 0 = negative class)\n",
        "y_true = [1, 0, 1, 1, 0]  # actual ground truth labels\n",
        "y_pred = [1, 0, 0, 1, 0]  # model predictions\n",
        "\n",
        "# Compute the metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.2f}\")\n",
        "print(f\"Precision: {prec:.2f}\")\n",
        "print(f\"Recall:    {rec:.2f}\")\n",
        "print(f\"F1-score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRFH_Ip8R9tu",
        "outputId": "db719242-5c5e-48b1-a73c-008fb358d369"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.80\n",
            "Precision: 1.00\n",
            "Recall:    0.67\n",
            "F1-score:  0.80\n"
          ]
        }
      ]
    }
  ]
}