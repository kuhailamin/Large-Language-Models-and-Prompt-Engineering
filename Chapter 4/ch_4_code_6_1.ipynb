{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm958bYA7WZA",
        "outputId": "789b8f12-523b-4222-ad90-cc8d9d56e199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.7194428443908691}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained BERT model for text classification\n",
        "model = \"bert-base-uncased\"\n",
        "\n",
        "# Initialize the text classification pipeline\n",
        "pipe = pipeline(\"text-classification\", model=model)\n",
        "\n",
        "# Example text\n",
        "text = \"I loved the movie! The acting was fantastic.\"\n",
        "\n",
        "# Use the model to classify the sample text\n",
        "result = pipe(text)\n",
        "\n",
        "label_mapping = { \"LABEL_0\": \"NEGATIVE\", \"LABEL_1\": \"POSITIVE\" }\n",
        "result[0][\"label\"] = label_mapping.get(result[0][\"label\"], result[0][\"label\"])\n",
        "\n",
        "# Print the prediction\n",
        "print(result)\n"
      ]
    }
  ]
}