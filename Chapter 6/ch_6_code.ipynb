{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cajlrqwlgROv"
      },
      "outputs": [],
      "source": [
        "1.\timport csv\n",
        "2.\tfrom langchain_core.documents import Document\n",
        "3.\tfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "4.\tfrom langchain_community.vectorstores import FAISS\n",
        "5.\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "6.\tfrom langchain.chains import RetrievalQA\n",
        "7.\n",
        "8.\t# Step 1: Load CSV file\n",
        "9.\tcsv_path = \"customer_support_tickets.csv\"\n",
        "10.\tdocuments = []\n",
        "11.\n",
        "12.\twith open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
        "13.\t   reader = csv.DictReader(csvfile)\n",
        "14.\t   for index, row in enumerate(reader):\n",
        "15.\t       # Build the content from subject, description, and resolution\n",
        "16.\t       content = f\"\"\"Customer Issue: {row['Ticket Subject']}\\nDescription: {row['Ticket Description']}\\nSupport Response: {row['Resolution']}\"\"\"\n",
        "17.\t       documents.append(Document(page_content=content, metadata={\"source\": f\"ticket_{index}\"}))\n",
        "18.\n",
        "19.\t# Step 2: Chunking\n",
        "20.\tsplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "21.\tchunks = splitter.split_documents(documents)\n",
        "22.\n",
        "23.\t# Step 3: Generate Embeddings\n",
        "24.\tembeddings = OpenAIEmbeddings(openai_api_key=\"API-KEY\")\n",
        "25.\tvectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "26.\tretriever = vectorstore.as_retriever()\n",
        "27.\n",
        "28.\t# Step 4: Use an LLM model\n",
        "29.\tllm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2, openai_api_key=\"API-KEY\")\n",
        "30.\n",
        "31.\t# Step 5: Use a RAG chain\n",
        "32.\trag_chain = RetrievalQA.from_chain_type(\n",
        "33.\t   llm=llm,\n",
        "34.\t   chain_type=\"stuff\",\n",
        "35.\t   retriever=retriever,\n",
        "36.\t   return_source_documents=True\n",
        "37.\t)\n",
        "38.\n",
        "39.\t# Step 6: Ask a query\n",
        "40.\tquery = \"What should I do if my software crashes?\"\n",
        "41.\tresult = rag_chain.invoke(query)\n",
        "42.\n",
        "43.\tprint(\"\\nAnswer:\\n\", result['result'])\n",
        "44.\tprint(\"\\nSources:\")\n",
        "45.\tfor doc in result['source_documents']:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1.\timport csv\n",
        "2.\tfrom langchain.chains import LLMChain\n",
        "3.\tfrom langchain.prompts import PromptTemplate\n",
        "4.\tfrom langchain_openai import ChatOpenAI\n",
        "5.\n",
        "6.\t# Step 1: load the dataset (reviews_dataset.csv)\n",
        "7.\tcsv_path = \"reviews_dataset.csv\"\n",
        "8.\tcomments = []\n",
        "9.\n",
        "10.\twith open(csv_path, newline='', encoding='cp1252') as csvfile:\n",
        "11.\t   reader = csv.DictReader(csvfile)\n",
        "12.\t   for row in reader:\n",
        "13.\t       if row.get(\"Comments\"):\n",
        "14.\t           comments.append(row[\"Comments\"])\n",
        "15.\n",
        "16.\t# Step 2: set up the LLM\n",
        "17.\tllm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2, openai_api_key=\"api key\")\n",
        "18.\n",
        "19.\t# Step 3: define a prompt that extracts polarity\n",
        "20.\tpolarity_prompt = PromptTemplate(\n",
        "21.\t   input_variables=[\"review\"],\n",
        "22.\t   template=\"\"\"\n",
        "23.\t   Analyze the sentiment polarity (Positive, Neutral, or Negative) of the following customer review:\n",
        "24.\t   Review: {review}\n",
        "25.\t   \"\"\"\n",
        "26.\t)\n",
        "27.\n",
        "28.\t# Step 3: define a prompt that extracts aspect-based sentiment (product, service, delivery)\n",
        "29.\taspect_prompt = PromptTemplate(\n",
        "30.\t   input_variables=[\"review\"],\n",
        "31.\t   template=\"\"\"\n",
        "32.\t   Analyze this customer review and extract sentiment for the following aspects: Product, Service, and Delivery.\n",
        "33.\t   Return the aspect sentiments in this format:\n",
        "34.\t   Product: [Positive/Neutral/Negative]\n",
        "35.\t   Service: [Positive/Neutral/Negative]\n",
        "36.\t   Delivery: [Positive/Neutral/Negative]\n",
        "37.\n",
        "38.\t   Review: {review}\n",
        "39.\t   \"\"\"\n",
        "40.\t)\n",
        "41.\n",
        "42.\t# Step 4: build the prompt chains\n",
        "43.\tpolarity_chain = polarity_prompt | llm\n",
        "44.\taspect_chain = aspect_prompt | llm\n",
        "45.\n",
        "46.\t# Step 5: run the analysis, and substitute the review argument\n",
        "47.\tfor index, comment in enumerate(comments, 1):\n",
        "48.\t   print(f\"\\n--- Review #{index} ---\")\n",
        "49.\t   print(\"Original Comment:\", comment)\n",
        "50.\n",
        "51.\t   polarity = polarity_chain.invoke({\"review\": comment}).content.strip()\n",
        "52.\t   aspects = aspect_chain.invoke({\"review\": comment}).content.strip()\n",
        "53.\n",
        "54.\t   print(\"Polarity:\", polarity)\n",
        "55.\t   print(\"Aspect-Based Sentiment:\\n\", aspects)\n"
      ],
      "metadata": {
        "id": "dEMc1CcRgVAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1.\tfrom langchain_core.documents import Document\n",
        "2.\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "3.\tfrom langchain_community.vectorstores import FAISS\n",
        "4.\tfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "5.\tfrom langchain.chains import RetrievalQA\n",
        "6.\n",
        "7.\t# Step 1: Load the policy document\n",
        "8.\twith open(\"policy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "9.\t   policy_text = f.read()\n",
        "10.\n",
        "11.\tdocuments = [Document(page_content=policy_text)]\n",
        "12.\n",
        "13.\t# Step 2: Chunk the policy document and create embeddings.\n",
        "14.\tsplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "15.\tchunks = splitter.split_documents(documents)\n",
        "16.\n",
        "17.\t# Step 3: create the vector index\n",
        "18.\tembeddings = OpenAIEmbeddings(openai_api_key=\"api-key\")\n",
        "19.\tvectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "20.\tretriever = vectorstore.as_retriever()\n",
        "21.\n",
        "22.\t# Step 4: set up the LLM\n",
        "23.\tllm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.3, openai_api_key=\"api-key\")\n",
        "24.\n",
        "25.\t# Step 5: Set up the RAG system\n",
        "26.\trag_chain = RetrievalQA.from_chain_type(\n",
        "27.\t   llm=llm,\n",
        "28.\t   chain_type=\"stuff\",  # simple concatenation of retrieved policy text\n",
        "29.\t   retriever=retriever,\n",
        "30.\t   return_source_documents=True\n",
        "31.\t)\n",
        "32.\n",
        "33.\t# Step 6: Show an example of handling a complaint\n",
        "34.\tcomplaint = \"My sushi arrived cold and 45 minutes late. Can I get a refund?\"\n",
        "35.\n",
        "36.\tresult = rag_chain.invoke(complaint)\n",
        "37.\n",
        "38.\t# Step 7: Show resolution and evidence\n",
        "39.\tprint(\"\\nResolution:\\n\", result[\"result\"])\n",
        "40.\tprint(\"\\nPolicy Evidence Used:\")\n",
        "41.\tfor doc in result['source_documents']:\n",
        "42.\t  print(f\"- {doc.page_content[:150]}...\")"
      ],
      "metadata": {
        "id": "MhjVEDGhgZpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code 6.X. Proactive support trigger for a delivery platform (early signals + LLM messaging)\n",
        "\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Load operational events (e.g., delivery delays)\n",
        "# events.csv columns:\n",
        "# timestamp, zone, delay_minutes, reason\n",
        "# -----------------------------\n",
        "events_path = \"events.csv\"\n",
        "zone_delay = {}  # latest delay per zone\n",
        "\n",
        "with open(events_path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        zone = row[\"zone\"].strip()\n",
        "        delay = int(row[\"delay_minutes\"])\n",
        "        ts = row[\"timestamp\"].strip()\n",
        "        # keep the latest delay snapshot per zone\n",
        "        if zone not in zone_delay or ts > zone_delay[zone][\"timestamp\"]:\n",
        "            zone_delay[zone] = {\"timestamp\": ts, \"delay_minutes\": delay, \"reason\": row[\"reason\"].strip()}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Load interaction signals (early warning cues)\n",
        "# interactions.csv columns:\n",
        "# timestamp, customer_id, zone, channel, text\n",
        "# channel examples: search, chat\n",
        "# -----------------------------\n",
        "interactions_path = \"interactions.csv\"\n",
        "customer_signals = defaultdict(list)\n",
        "\n",
        "with open(interactions_path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        customer_id = row[\"customer_id\"].strip()\n",
        "        zone = row[\"zone\"].strip()\n",
        "        customer_signals[customer_id].append(\n",
        "            {\n",
        "                \"timestamp\": row[\"timestamp\"].strip(),\n",
        "                \"zone\": zone,\n",
        "                \"channel\": row[\"channel\"].strip(),\n",
        "                \"text\": row[\"text\"].strip(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Simple heuristic pre-filter (cheap, non-LLM)\n",
        "# We only send LLM work for customers in zones with active delays,\n",
        "# and whose language suggests uncertainty or repeated checking.\n",
        "# -----------------------------\n",
        "RISK_PHRASES = [\n",
        "    \"where is my order\",\n",
        "    \"still waiting\",\n",
        "    \"late\",\n",
        "    \"delay\",\n",
        "    \"refund\",\n",
        "    \"cancel\",\n",
        "    \"what's happening\",\n",
        "    \"no update\",\n",
        "    \"taking so long\",\n",
        "    \"again\",\n",
        "    \"third time\",\n",
        "    \"not responding\",\n",
        "]\n",
        "\n",
        "def heuristic_risk_score(signals: list[dict]) -> int:\n",
        "    score = 0\n",
        "    # more interactions in short time -> higher risk\n",
        "    score += min(len(signals), 5)\n",
        "    # keywords -> higher risk\n",
        "    for s in signals:\n",
        "        t = s[\"text\"].lower()\n",
        "        if any(p in t for p in RISK_PHRASES):\n",
        "            score += 2\n",
        "        # searches are “early” signals (often precede complaints)\n",
        "        if s[\"channel\"].lower() == \"search\":\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Set up the LLM (used only for (a) risk classification and (b) message generation)\n",
        "# -----------------------------\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2, openai_api_key=\"API-KEY\")\n",
        "\n",
        "risk_prompt = PromptTemplate(\n",
        "    input_variables=[\"delay_minutes\", \"reason\", \"signals\"],\n",
        "    template=\"\"\"\n",
        "You are a customer support analyst for a food delivery platform.\n",
        "Given operational delay context and customer interaction signals, classify the customer's risk of complaining in the next hour.\n",
        "\n",
        "Delay context:\n",
        "- delay_minutes: {delay_minutes}\n",
        "- reason: {reason}\n",
        "\n",
        "Customer signals (latest first):\n",
        "{signals}\n",
        "\n",
        "Return ONLY one of: LOW, MEDIUM, HIGH\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "message_prompt = PromptTemplate(\n",
        "    input_variables=[\"delay_minutes\", \"reason\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful customer support assistant for a food delivery platform.\n",
        "Write a proactive message to a customer affected by a delivery delay.\n",
        "\n",
        "Constraints:\n",
        "- Be brief (2–4 sentences).\n",
        "- Acknowledge the delay without blaming the customer.\n",
        "- Provide a realistic expectation (mention the delay_minutes).\n",
        "- Offer a helpful next step (e.g., tracking, support contact).\n",
        "- Tone: calm, respectful, empathetic.\n",
        "\n",
        "Delay minutes: {delay_minutes}\n",
        "Reason: {reason}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "risk_chain = risk_prompt | llm\n",
        "message_chain = message_prompt | llm\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Run proactive detection + generate messages\n",
        "# -----------------------------\n",
        "PROACTIVE_THRESHOLD = 7  # heuristic threshold to consider running LLM classification\n",
        "\n",
        "proactive_actions = []\n",
        "\n",
        "for customer_id, signals in customer_signals.items():\n",
        "    # sort signals newest first\n",
        "    signals_sorted = sorted(signals, key=lambda x: x[\"timestamp\"], reverse=True)\n",
        "    zone = signals_sorted[0][\"zone\"]\n",
        "\n",
        "    # only consider customers in zones with a known active delay snapshot\n",
        "    if zone not in zone_delay:\n",
        "        continue\n",
        "\n",
        "    delay_minutes = zone_delay[zone][\"delay_minutes\"]\n",
        "    reason = zone_delay[zone][\"reason\"]\n",
        "\n",
        "    # cheap heuristic filter first\n",
        "    h_score = heuristic_risk_score(signals_sorted)\n",
        "    if h_score < PROACTIVE_THRESHOLD:\n",
        "        continue\n",
        "\n",
        "    # format signals for the LLM\n",
        "    signals_text = \"\\n\".join(\n",
        "        [f\"- [{s['timestamp']}] ({s['channel']}) {s['text']}\" for s in signals_sorted[:8]]\n",
        "    )\n",
        "\n",
        "    # LLM risk classification\n",
        "    risk_level = risk_chain.invoke(\n",
        "        {\n",
        "            \"delay_minutes\": delay_minutes,\n",
        "            \"reason\": reason,\n",
        "            \"signals\": signals_text,\n",
        "        }\n",
        "    ).content.strip().upper()\n",
        "\n",
        "    # only send proactive messages for medium/high risk\n",
        "    if risk_level in {\"MEDIUM\", \"HIGH\"}:\n",
        "        proactive_message = message_chain.invoke(\n",
        "            {\n",
        "                \"delay_minutes\": delay_minutes,\n",
        "                \"reason\": reason,\n",
        "            }\n",
        "        ).content.strip()\n",
        "\n",
        "        proactive_actions.append(\n",
        "            {\n",
        "                \"customer_id\": customer_id,\n",
        "                \"zone\": zone,\n",
        "                \"delay_minutes\": delay_minutes,\n",
        "                \"risk_level\": risk_level,\n",
        "                \"message\": proactive_message,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Output (in production, you would dispatch via SMS/push/in-app message)\n",
        "# -----------------------------\n",
        "print(\"\\n=== Proactive Support Actions ===\")\n",
        "for a in proactive_actions:\n",
        "    print(f\"\\nCustomer: {a['customer_id']} | Zone: {a['zone']} | Delay: {a['delay_minutes']} min | Risk: {a['risk_level']}\")\n",
        "    print(\"Proactive Message:\")\n",
        "    print(a[\"message\"])"
      ],
      "metadata": {
        "id": "6ne47afrjmDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}